"""
Shared test fixtures for the refresh system tests.

Provides factories for generating realistic JSONL transcripts and
common fixtures used across multiple test modules.
"""

import json
import os
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Any
from unittest.mock import patch

import pytest


# =============================================================================
# Transcript Line Factories
# =============================================================================

def make_user_message(
    content: str,
    timestamp: str | None = None,
    session_id: str = "test-session-123",
) -> dict[str, Any]:
    """
    Create a user message line for JSONL transcript.

    Args:
        content: The user's message text
        timestamp: ISO timestamp (generated if not provided)
        session_id: Session ID for the message

    Returns:
        Dict suitable for JSON serialization as JSONL line
    """
    if timestamp is None:
        timestamp = datetime.now(timezone.utc).isoformat()

    return {
        "type": "user",
        "sessionId": session_id,
        "message": {
            "role": "user",
            "content": content,
        },
        "timestamp": timestamp,
    }


def make_assistant_message(
    content: str | list[dict[str, Any]],
    timestamp: str | None = None,
) -> dict[str, Any]:
    """
    Create an assistant message line for JSONL transcript.

    Args:
        content: Text content or list of content blocks
        timestamp: ISO timestamp (generated if not provided)

    Returns:
        Dict suitable for JSON serialization as JSONL line
    """
    if timestamp is None:
        timestamp = datetime.now(timezone.utc).isoformat()

    # Handle string content
    if isinstance(content, str):
        content_blocks = [{"type": "text", "text": content}]
    else:
        content_blocks = content

    return {
        "type": "assistant",
        "message": {
            "role": "assistant",
            "content": content_blocks,
        },
        "timestamp": timestamp,
    }


def make_tool_use_block(
    name: str,
    input_data: dict[str, Any],
    tool_use_id: str = "tool-123",
) -> dict[str, Any]:
    """
    Create a tool_use content block for assistant messages.

    Args:
        name: Tool name (e.g., "Task", "Read", "Write")
        input_data: Tool input parameters
        tool_use_id: Unique ID for the tool call

    Returns:
        Dict representing a tool_use content block
    """
    return {
        "type": "tool_use",
        "id": tool_use_id,
        "name": name,
        "input": input_data,
    }


def make_task_call(
    subagent_type: str,
    prompt: str,
    tool_use_id: str = "task-123",
) -> dict[str, Any]:
    """
    Create a Task tool call block for invoking PACT agents.

    Args:
        subagent_type: Agent type (e.g., "pact-backend-coder")
        prompt: The prompt sent to the agent
        tool_use_id: Unique ID for the tool call

    Returns:
        Dict representing a Task tool_use content block
    """
    return make_tool_use_block(
        name="Task",
        input_data={
            "subagent_type": subagent_type,
            "prompt": prompt,
            "run_in_background": True,
        },
        tool_use_id=tool_use_id,
    )


# =============================================================================
# Transcript Factories
# =============================================================================

def create_transcript_lines(lines: list[dict[str, Any]]) -> str:
    """
    Convert a list of message dicts to JSONL string.

    Args:
        lines: List of message dictionaries

    Returns:
        JSONL-formatted string (one JSON object per line)
    """
    return "\n".join(json.dumps(line) for line in lines)


def create_peer_review_transcript(
    step: str = "recommendations",
    include_pr_number: int | None = 64,
    include_termination: bool = False,
    include_pending_question: bool = True,
) -> str:
    """
    Generate a realistic peer-review workflow transcript.

    Args:
        step: Current workflow step (e.g., "recommendations", "merge-ready")
        include_pr_number: PR number to include in context (None to omit)
        include_termination: Whether to add termination signal
        include_pending_question: Whether to add pending AskUserQuestion

    Returns:
        JSONL string representing the transcript
    """
    lines = []
    base_time = "2025-01-22T12:00:00Z"

    # User triggers peer-review
    lines.append(make_user_message(
        "/PACT:peer-review",
        timestamp=base_time,
    ))

    # Assistant acknowledges and starts workflow
    lines.append(make_assistant_message(
        f"Starting peer-review workflow. Creating PR #{include_pr_number or 'XX'}...",
        timestamp="2025-01-22T12:00:05Z",
    ))

    # Commit phase
    lines.append(make_assistant_message(
        "Commit phase: committing changes...",
        timestamp="2025-01-22T12:00:10Z",
    ))

    # Create-PR phase
    if include_pr_number:
        lines.append(make_assistant_message(
            f"create-pr phase: PR #{include_pr_number} created successfully.",
            timestamp="2025-01-22T12:00:20Z",
        ))

    # Invoke reviewers
    lines.append(make_assistant_message(
        content=[
            {"type": "text", "text": "invoke-reviewers: Invoking review agents..."},
            make_task_call("pact-architect", "Review PR design coherence", "task-arch"),
            make_task_call("pact-test-engineer", "Review test coverage", "task-test"),
            make_task_call("pact-backend-coder", "Review implementation quality", "task-backend"),
        ],
        timestamp="2025-01-22T12:00:30Z",
    ))

    # Synthesize findings
    lines.append(make_assistant_message(
        "synthesize: All reviewers completed. No blocking issues. 0 minor, 1 future recommendation.",
        timestamp="2025-01-22T12:01:00Z",
    ))

    # Recommendations step with pending question
    if step in ["recommendations", "pre-recommendation-prompt", "merge-ready"]:
        if include_pending_question:
            lines.append(make_assistant_message(
                "recommendations phase: AskUserQuestion: Would you like to review the minor and future recommendations before merging?",
                timestamp="2025-01-22T12:01:10Z",
            ))
        else:
            lines.append(make_assistant_message(
                "recommendations phase: Presenting recommendations to user.",
                timestamp="2025-01-22T12:01:10Z",
            ))

    # Merge-ready step
    if step == "merge-ready":
        lines.append(make_assistant_message(
            "merge-ready: All checks passed. Awaiting user approval to merge.",
            timestamp="2025-01-22T12:01:30Z",
        ))

    # Termination
    if include_termination:
        lines.append(make_assistant_message(
            f"PR #{include_pr_number or 'XX'} has been merged successfully.",
            timestamp="2025-01-22T12:02:00Z",
        ))

    return create_transcript_lines(lines)


def create_orchestrate_transcript(
    phase: str = "code",
    include_task: str = "implement auth",
    include_agent_calls: bool = True,
    include_termination: bool = False,
) -> str:
    """
    Generate a realistic orchestrate workflow transcript.

    Args:
        phase: Current phase (variety-assess, prepare, architect, code, test)
        include_task: Task description
        include_agent_calls: Whether to include Task calls to PACT agents
        include_termination: Whether to add termination signal

    Returns:
        JSONL string representing the transcript
    """
    lines = []

    # User triggers orchestrate
    lines.append(make_user_message(
        f"/PACT:orchestrate {include_task}",
        timestamp="2025-01-22T10:00:00Z",
    ))

    # Variety assessment
    lines.append(make_assistant_message(
        f"variety-assess: Analyzing task: {include_task}. Estimated complexity: medium.",
        timestamp="2025-01-22T10:00:05Z",
    ))

    # Prepare phase
    if phase in ["prepare", "architect", "code", "test"]:
        content_blocks = [
            {"type": "text", "text": "prepare phase: Invoking preparer for requirements gathering."},
        ]
        if include_agent_calls:
            content_blocks.append(make_task_call("pact-preparer", "Research auth patterns", "task-prep"))
        lines.append(make_assistant_message(content_blocks, "2025-01-22T10:00:15Z"))

    # Architect phase
    if phase in ["architect", "code", "test"]:
        content_blocks = [
            {"type": "text", "text": "architect phase: Designing component structure."},
        ]
        if include_agent_calls:
            content_blocks.append(make_task_call("pact-architect", "Design auth module", "task-arch"))
        lines.append(make_assistant_message(content_blocks, "2025-01-22T10:01:00Z"))

    # Code phase
    if phase in ["code", "test"]:
        content_blocks = [
            {"type": "text", "text": "code phase: Starting implementation."},
        ]
        if include_agent_calls:
            content_blocks.append(make_task_call("pact-backend-coder", "Implement auth endpoint", "task-code"))
        lines.append(make_assistant_message(content_blocks, "2025-01-22T10:02:00Z"))

    # Test phase
    if phase == "test":
        content_blocks = [
            {"type": "text", "text": "test phase: Running comprehensive tests."},
        ]
        if include_agent_calls:
            content_blocks.append(make_task_call("pact-test-engineer", "Test auth module", "task-test"))
        lines.append(make_assistant_message(content_blocks, "2025-01-22T10:03:00Z"))

    # Termination
    if include_termination:
        lines.append(make_assistant_message(
            "all phases complete. IMPLEMENTED: Auth endpoint is ready.",
            timestamp="2025-01-22T10:05:00Z",
        ))

    return create_transcript_lines(lines)


def create_no_workflow_transcript() -> str:
    """
    Generate a transcript with no active PACT workflow.

    Returns:
        JSONL string with normal conversation (no /PACT:* triggers)
    """
    lines = [
        make_user_message("Hello, can you help me understand this codebase?"),
        make_assistant_message("Of course! Let me explore the project structure..."),
        make_user_message("What's in the hooks directory?"),
        make_assistant_message([
            {"type": "text", "text": "Looking at the hooks directory..."},
            make_tool_use_block("Read", {"file_path": "/project/hooks/hooks.json"}),
        ]),
        make_assistant_message("The hooks directory contains several Python hooks for Claude Code integration."),
    ]
    return create_transcript_lines(lines)


def create_terminated_workflow_transcript() -> str:
    """
    Generate a transcript with a completed (terminated) workflow.

    Returns:
        JSONL string with peer-review that has been merged
    """
    return create_peer_review_transcript(
        step="merge-ready",
        include_pr_number=99,
        include_termination=True,
        include_pending_question=False,
    )


def create_malformed_transcript() -> str:
    """
    Generate a transcript with malformed JSONL lines.

    Returns:
        JSONL string with some invalid lines
    """
    valid_line = make_user_message("/PACT:peer-review")
    lines = [
        json.dumps(valid_line),
        "{ invalid json",
        "",  # Empty line
        "not json at all",
        json.dumps(make_assistant_message("Starting workflow...")),
        '{"type": "unknown_type", "data": {}}',  # Unknown type
    ]
    return "\n".join(lines)


# =============================================================================
# Pytest Fixtures
# =============================================================================

@pytest.fixture
def tmp_transcript(tmp_path: Path):
    """
    Factory fixture to create temporary transcript files.

    Returns:
        Function that creates a temp JSONL file and returns its path
    """
    def _create(content: str, filename: str = "session.jsonl") -> Path:
        # Create directory structure mimicking Claude's format
        projects_dir = tmp_path / ".claude" / "projects"
        encoded_path = "-test-project"
        session_dir = projects_dir / encoded_path / "session-uuid"
        session_dir.mkdir(parents=True, exist_ok=True)

        transcript_path = session_dir / filename
        transcript_path.write_text(content, encoding="utf-8")
        return transcript_path

    return _create


@pytest.fixture
def mock_env():
    """
    Fixture to mock environment variables.

    Returns:
        Context manager function for patching environment
    """
    def _mock(session_id: str = "test-session-123", project_dir: str = "/test/project"):
        return patch.dict(os.environ, {
            "CLAUDE_SESSION_ID": session_id,
            "CLAUDE_PROJECT_DIR": project_dir,
        })
    return _mock


@pytest.fixture
def tmp_checkpoint_dir(tmp_path: Path):
    """
    Fixture providing a temporary checkpoint directory.

    Returns:
        Path to the temporary checkpoint directory
    """
    checkpoint_dir = tmp_path / ".claude" / "pact-refresh"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    return checkpoint_dir


@pytest.fixture
def sample_checkpoint() -> dict[str, Any]:
    """
    Fixture providing a sample valid checkpoint.

    Returns:
        Dict representing a valid checkpoint
    """
    return {
        "version": "1.0",
        "session_id": "test-session-123",
        "workflow": {
            "name": "peer-review",
            "id": "pr-64",
            "started_at": "2025-01-22T12:00:00Z",
        },
        "step": {
            "name": "recommendations",
            "sequence": 5,
            "started_at": "2025-01-22T12:01:10Z",
        },
        "pending_action": {
            "type": "AskUserQuestion",
            "instruction": "Would you like to review the recommendations?",
            "data": {},
        },
        "context": {
            "pr_number": 64,
            "has_blocking": False,
            "minor_count": 0,
            "future_count": 1,
        },
        "extraction": {
            "confidence": 0.9,
            "notes": "clear trigger, step: recommendations, 3 agent call(s)",
            "transcript_lines_scanned": 150,
        },
        "created_at": "2025-01-22T12:05:30Z",
    }


# =============================================================================
# Fixture Files
# =============================================================================

@pytest.fixture
def peer_review_mid_workflow_transcript() -> str:
    """Fixture returning a peer-review transcript mid-workflow."""
    return create_peer_review_transcript(
        step="recommendations",
        include_pr_number=64,
        include_termination=False,
        include_pending_question=True,
    )


@pytest.fixture
def orchestrate_code_phase_transcript() -> str:
    """Fixture returning an orchestrate transcript in CODE phase."""
    return create_orchestrate_transcript(
        phase="code",
        include_task="implement auth endpoint",
        include_agent_calls=True,
        include_termination=False,
    )


@pytest.fixture
def no_workflow_transcript() -> str:
    """Fixture returning a transcript with no active workflow."""
    return create_no_workflow_transcript()


@pytest.fixture
def terminated_workflow_transcript() -> str:
    """Fixture returning a transcript with completed workflow."""
    return create_terminated_workflow_transcript()
